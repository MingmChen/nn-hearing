# Training a Neural Network to Recognize Speaker Voices

A small experiment that uses TensorFlow to train a convolutional neural network (CNN) on speaker voices. Training is done with .wav files with 2-3 seconds of a person talking, and again with the same speaker voices distorted to simulate deafened hearing. 

The unmodified and vocoded sound files are contained in the [Clear](Clear) and [Vocoded](Vocoded) directories, respectively. Loadsound scripts parse these files into a pickled dataset, and the cnn scripts load that data set to train the CNN and evaluate its accuracy of recognizing speaker voices.

## Files

[Clear](Clear):
.wav audio files of people speaking 2-3 seconds.

[Vocoded](Vocoded):
.wav audio files of people speaking 2-3 seconds. Vocoded to simulate hearing with a cochlear implant.

[loadsound.py](loadsound.py)
Load a batch of sound files from either Vocoded or Clear directory. Store the .wav data, mel spectrogram, and unique file identifier for each file in a pandas dataframe and write dataframe to a pickle.

[loadsound_advanced.py](loadsound_advanced.py)
Same as loadsound.py, except instead of a unique file identifier include a speaker title for each file.

[cnn_basic.py](cnn_basic.py)
Define, implement, and evaluate a basic neural network that uses the pickle generated from loadsound.py as data. The neural net is intended to train on the mel spectrograms of the .wav files and their corresponding labels and then test on those same files.

[cnn_advanced.py](cnn_advanced.py)
Same as cnn_basic.py, except that it trains on speaker titles instead of file labels, and has a separate set of data for training and testing. Uses pickles generated by loadsound_advanced.py.
